{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitoraugusto1993/ml-projects/blob/main/LLM/GPT%20Summarization/Summarization_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIs4bngsPm8"
      },
      "source": [
        "## TASK 1: Install and setup LangChain\n",
        "Welcome to this project notebook, which will serve as your guide to constructing your inaugural Generative AI application. Within this notebook, you'll encounter concise descriptions of each task to enhance your comprehension of the sequence. Our initial task commences with the importation of essential libraries required for this project.\n",
        "\n",
        "***Note: Before importing the libraries please ensure that all the library modules such as Langchain, Streamlit, PyPdf and other required libraries are installed on this notebook by running the pip command as shown below.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wnvxw5okxYjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc99aab4-aee3-4555-d878-470f89efb83e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/810.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/810.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m809.0/810.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.29 (from langchain)\n",
            "  Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n",
            "  Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.33)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.1.31)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.0.7)\n",
            "Installing collected packages: h11, tiktoken, httpcore, httpx, openai, langchain_openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 langchain_openai-0.0.8 openai-1.14.2 tiktoken-0.6.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.1.0\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.32.2-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.10.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.42 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.32.2 watchdog-4.0.0\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "#For installing the Langchain module associated with OpenAI LLM model\n",
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "#For installing the Python library responsible for PDF upload\n",
        "!pip install pypdf\n",
        "#For installing the library responsible for web app development\n",
        "!pip install streamlit\n",
        "#For installing tokeniser library that asists with converting text strings into tokens recognizable by OpenAI models\n",
        "!pip install tiktoken\n",
        "#For installing the library used for invoking the environment file containing secret API key\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bJ0AR0gorTlF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import streamlit as st\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load OpenAI API Key to access LLM model\n",
        "\n",
        "1. Go to https://platform.openai.com/api-keys\n",
        "2. Click on the '+ Creat new secret key button'\n",
        "3. Enter an identifier name(optional) and click on the \"Create secret key\" button\n",
        "4. Copy the API key to be used in the API.env file that you need to upload to Google Colab environment"
      ],
      "metadata": {
        "id": "bvtUejaS8v8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload Env file with the OpenAI API key\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "nocHXlQzmy_a",
        "outputId": "deff99ac-b213-4ccd-e3a5-56952ec44ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-07564620-3730-4381-b0e1-445e6b1843d1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-07564620-3730-4381-b0e1-445e6b1843d1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving API.env to API.env\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'API.env': b'/content# cat .env\\r\\nOPENAI_API_KEY=\"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\"\\r\\n/content# '}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qtlBVZonvkf8"
      },
      "outputs": [],
      "source": [
        "# Load the .env file and invoke the secret API key from the file\n",
        "dotenv.load_dotenv('API.env')\n",
        "OpenAI.api_key = os.getenv(\"OPEN_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRjptllP7Xgb"
      },
      "source": [
        "# Load PDF file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload my article pdf file\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "1oXqR-FDnNAa",
        "outputId": "7e4327f8-7b09-4bc2-fe15-a5e41f429c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-400b8df9-8409-48a9-9430-8f9889adf66d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-400b8df9-8409-48a9-9430-8f9889adf66d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Addressing_Lane_Keeping_and_Intersections_using_Deep_Conditional_Reinforcement_Learning.pdf to Addressing_Lane_Keeping_and_Intersections_using_Deep_Conditional_Reinforcement_Learning.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FL4FtXMkvocT"
      },
      "outputs": [],
      "source": [
        "pdf_url = \"Addressing_Lane_Keeping_and_Intersections_using_Deep_Conditional_Reinforcement_Learning.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(pdf_url)\n",
        "pages = loader.load_and_split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HEGnGRAe_op2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4e854d-47e5-4ff2-a9fb-ff984c326ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Addressing Lane Keeping and Intersections using Deep Conditional\n",
            "Reinforcement Learning\n",
            "V´ıtor A. S. Silva1and Valdir Grassi Jr1\n",
            "Abstract — End-to-end deep reinforcement learning (DRL)\n",
            "methods have been largely used to solve self-driving tasks.\n",
            "However, they usually deal only with simple problems, such\n",
            "as lane keeping. When it is needed to solve more complex\n",
            "tasks, traditional DRL algorithms fails. In this work we combine\n",
            "Conditional Learning and Proximal Policy Optimization (PPO)\n",
            "to solve the problem of turning at intersection and lane\n",
            "keeping in an end-to-end fashion. In our approach we trained\n",
            "three PPO sub-policies to perform right and left turns and\n",
            "follow lane. The sub-policies are activated once at a time\n",
            "according to a command sent by the local planner. We also used\n",
            "three different image transformation to verify their impact on\n",
            "learning speed and generalization. Experiments were conducted\n",
            "in an urban scenario composed by several intersections of\n",
            "type T on CARLA Simulator. Results show that our approach\n",
            "is feasible and presents good performance on accomplishing\n",
            "the goal. Moreover, we could conﬁrm that properly choosing\n",
            "an image transformation can improve sample efﬁciency and\n",
            "generalization capability.\n",
            "I. I NTRODUCTION\n",
            "Autonomous vehicles, when they are well developed and\n",
            "used on a large scale, will have the potential to provide\n",
            "greater trafﬁc safety, solving the main causes of accidents [1].\n",
            "In addition, this technology would afford greater mobility for\n",
            "elderly and people with physical and visual disability, greater\n",
            "efﬁciency on energy consumption, and increase productivity\n",
            "of time spent on trafﬁc [1], [2].\n",
            "Researches on autonomous driving rely on two main\n",
            "approaches to solve self-driving tasks. The ﬁrst, referred as\n",
            "mediated approach, uses separated and specialized modules\n",
            "that communicate in a pipeline, from sensory to the actuators\n",
            "control [1]. The common modules used in this approach are:\n",
            "perception, localization and mapping, planning and decision\n",
            "making, and control [1]–[4]. The second approach is named\n",
            "end-to-end, in which the control commands of actuators are\n",
            "mapped directly from the observations, commonly relying on\n",
            "machine learning techniques [2], [5], being deep reinforce-\n",
            "ment learning (DRL) and imitation learning (IL) the two\n",
            "most employed [3].\n",
            "With success in solving games with super-human results,\n",
            "reinforcement learning proved to be a viable approach to\n",
            "*This work was ﬁnanced in part by the Brazilian National Council for\n",
            "Scientiﬁc and Technological Development - CNPq (grant 465755/2014-3),\n",
            "by the Coordination of Improvement of Higher Education Personnel – Brazil\n",
            "- CAPES (Finance Code 001 and 88887.136349/2017-00), and the S ˜ao Paulo\n",
            "Research Foundation - FAPESP, Brazil (grant 2014/50851-0).\n",
            "1The authors are with the Department of Electrical and Com-\n",
            "puter Engineering, S ˜ao Carlos School of Engineering, University of\n",
            "S˜ao Paulo, S ˜ao Carlos, Brazil. vitor.augustoss@usp.br;\n",
            "vgrassi@usp.br.\n",
            "978-1-6654-0761-8/21/$31.00 ©2021 IEEEaddress self-driving tasks. Kendall et al. [6] were the ﬁrst to\n",
            "use RL in this context, making a vehicle to drive following a\n",
            "lane only with aid of a monocular camera. Posteriorly, many\n",
            "other works using a RL method in an end-to-end fashion\n",
            "were done, such as [3], [7], [8], and others.\n",
            "Among the reasons that one can choose an end-to-end\n",
            "approach over a mediated to solve navigation tasks are that\n",
            "the latter: (i) lacks on generalization for new scenarios and\n",
            "tasks, (ii) is prone to error propagation through pipeline, (iii)\n",
            "is more complex to design, and (iv) can produce conservative\n",
            "driving policies due to human heuristics, since it is hand-\n",
            "engineered [3].\n",
            "However, end-to-end frameworks also have some draw-\n",
            "backs. Due to the fact that this approach relies mainly on the\n",
            "use of deep neural networks, it is not possible to explain how\n",
            "the policy learned works (lack of interpretability) [3], [9].\n",
            "Another shortcoming is that it presents good performance\n"
          ]
        }
      ],
      "source": [
        "#number of pages\n",
        "len(pages)\n",
        "\n",
        "#view page content\n",
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyDp0eJj_hNd"
      },
      "source": [
        "## TASK 2: Define the summarize pdf function\n",
        "Define the main function that will take pdf file path as an input and generate a summary of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kON5tJmk09Mp"
      },
      "outputs": [],
      "source": [
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, chain_type='map_reduce'):\n",
        "\n",
        "    #Instantiate LLM model gpt-3.5-turbo\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OpenAI.api_key)\n",
        "\n",
        "    #Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load_and_split()\n",
        "\n",
        "    #Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    #Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    #Summarize the chunks (map_reduce method takes longer to execute)\n",
        "    chain = load_summarize_chain(llm, chain_type=chain_type)\n",
        "\n",
        "    #Return the summary\n",
        "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "    return summary['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zZsZ5TGMM-WS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7ff95b-f5f7-4f17-ae91-cc9373a6523c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The study addresses the challenges of lane keeping and decision-making at intersections in self-driving vehicles using deep conditional reinforcement learning. The researchers combined Conditional Learning and Proximal Policy Optimization (PPO) to train sub-policies for turning at intersections and lane keeping. Three different image transformations were used to improve learning speed and generalization. Experiments were conducted in an urban scenario using the CARLA Simulator, showing that the approach is feasible and effective. The study highlights the importance of specialized sub-policies and image augmentation in improving the performance of autonomous driving agents. Future work aims to enhance decision-making capabilities and collision avoidance in autonomous vehicles.\n"
          ]
        }
      ],
      "source": [
        "#print summary by using chain type stuff\n",
        "#Chunk size and chunk overlap values set to random value\n",
        "\n",
        "print(summarize_pdf(pdf_url, 1000, 20, chain_type='stuff'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The study addresses the challenges of lane keeping and decision-making at intersections in self-driving vehicles using deep conditional reinforcement learning. The researchers combined Conditional Learning and Proximal Policy Optimization (PPO) to train sub-policies for turning at intersections and lane keeping. Three different image transformations were used to improve learning speed and generalization. Experiments were conducted in an urban scenario using the CARLA Simulator, showing that the approach is feasible and effective. The study highlights the importance of specialized sub-policies and image augmentation in improving the performance of autonomous driving agents. Future work aims to enhance decision-making capabilities and collision avoidance in autonomous vehicles."
      ],
      "metadata": {
        "id": "uUvYFpVRBBUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print summary by using chain type map_reduce\n",
        "#Chunk size and chunk overlap values set to random value\n",
        "\n",
        "print(summarize_pdf(pdf_url, 1000, 20, chain_type='map_reduce'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgFBiv0mBkSQ",
        "outputId": "bb2284b7-f319-4606-dba6-b0bc6e24c2eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The study explores the use of deep conditional reinforcement learning for complex self-driving tasks like turning at intersections and lane keeping. Researchers combined Conditional Learning and Proximal Policy Optimization (PPO) to train sub-policies for different maneuvers. Experiments were conducted in an urban scenario using the CARLA Simulator, showing the feasibility and performance of the approach. The study emphasizes the benefits of autonomous vehicles and discusses the advantages and disadvantages of end-to-end versus mediated approaches in robotics. The use of reinforcement learning in self-driving tasks, image augmentation, and the importance of data efficiency and generalization are also highlighted. The study proposes a route planner using PPO to address ambiguity at intersections and improve decision-making. Results show that deep conditional reinforcement learning can effectively solve ambiguity in autonomous driving, with future work focusing on enhancing decision-making and collision avoidance capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The study explores the use of deep conditional reinforcement learning for complex self-driving tasks like turning at intersections and lane keeping. Researchers combined Conditional Learning and Proximal Policy Optimization (PPO) to train sub-policies for different maneuvers. Experiments were conducted in an urban scenario using the CARLA Simulator, showing the feasibility and performance of the approach. The study emphasizes the benefits of autonomous vehicles and discusses the advantages and disadvantages of end-to-end versus mediated approaches in robotics. The use of reinforcement learning in self-driving tasks, image augmentation, and the importance of data efficiency and generalization are also highlighted. The study proposes a route planner using PPO to address ambiguity at intersections and improve decision-making. Results show that deep conditional reinforcement learning can effectively solve ambiguity in autonomous driving, with future work focusing on enhancing decision-making and collision avoidance capabilities."
      ],
      "metadata": {
        "id": "oNMKrJSXCYen"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvnt8H-L3Yzb"
      },
      "source": [
        "## TASK 3: Add Prompt template to the summarizer function\n",
        "Leveraging prompt templates to extract key information from the reserach paper in more guided manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s2a87sV4Hkl"
      },
      "source": [
        "## Define Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template for Stuffing chain type"
      ],
      "metadata": {
        "id": "4Wy1_iJlfQRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zac7tS6p3YKU"
      },
      "outputs": [],
      "source": [
        "map_prompt_template = \"\"\"\n",
        "                       Write a summary of the research paper for an\n",
        "                       artficial intelligence researcher that includes\n",
        "                       main points and any important details in bullet points.{text}\n",
        "                      \"\"\"\n",
        "\n",
        "map_prompt = PromptTemplate(input_variables=[\"text\"], template=map_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Combo Template for Map_Reduce chain type"
      ],
      "metadata": {
        "id": "Il7HTR6pfjsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combine_prompt_template = \"\"\"\n",
        "                            You will be given main points and any important details of a research paper in bullet points.\n",
        "                            Your goal is to give a final summary of the main research topic and fidings\n",
        "                            which will be useful to an artificial intelligence researcher\n",
        "                            to grasp what was done during the research work\n",
        "\n",
        "```{text}```\n",
        "FINAL SUMMARY:\n",
        "\"\"\"\n",
        "\n",
        "combine_prompt = PromptTemplate(\n",
        "      input_variables=[\"text\"],\n",
        "      template=combine_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "AaZoSIMDfMPu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6xyYdjJZEJJs"
      },
      "outputs": [],
      "source": [
        "#Modify the custom function to add the prompt templates\n",
        "\n",
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, map_prompt, combine_prompt, chain_type='map_reduce'):\n",
        "\n",
        "    #Instantiate LLM model gpt-3.5-turbo\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OpenAI.api_key)\n",
        "\n",
        "    #Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load_and_split()\n",
        "\n",
        "    #Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    #Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    #Summarize the chunks (map_reduce method takes longer to execute)\n",
        "    chain = load_summarize_chain(llm, chain_type=chain_type, map_prompt=map_prompt, combine_prompt=combine_prompt)\n",
        "\n",
        "    #Return the summary\n",
        "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "    return summary['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print summary using the map_prompt and combine prompt\n",
        "#Increasing the chunk size value might reduce the overall summarization time with map_reduce method\n",
        "print(summarize_pdf(pdf_url, 2000, 100, map_prompt, combine_prompt))"
      ],
      "metadata": {
        "id": "_2bNEpmcPG0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2f355b-5dbc-4e5f-c490-abf9af774542"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The research paper addresses the limitations of traditional deep reinforcement learning algorithms in solving complex self-driving tasks, specifically turning at intersections. The authors propose a novel approach that combines Conditional Learning and Proximal Policy Optimization (PPO) to address this challenge. The study conducted experiments in an urban scenario with T-type intersections using the CARLA Simulator, showing that the proposed approach is feasible and performs well in achieving the goal of turning at intersections. The research also highlights the potential benefits of autonomous vehicles in improving traffic safety, mobility for elderly and disabled individuals, energy efficiency, and productivity. The paper discusses the lack of interpretability in AI systems and how it affects performance, proposing solutions such as Conditional Imitation Learning and augmentation data. The use of Proximal Policy Optimization (PPO) and augmentation data is emphasized to improve sample efficiency and generalization in end-to-end self-driving tasks. The research also delves into the training process, evaluation, and performance analysis of different reinforcement learning agents, showcasing the effectiveness of PPO agents with augmentation in improving driving performance in a simulated environment. Overall, the paper presents a comprehensive approach to enhancing autonomous driving capabilities through artificial intelligence techniques, contributing to the advancement of self-driving technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The research paper addresses the limitations of traditional deep reinforcement learning algorithms in solving complex self-driving tasks, specifically turning at intersections. The authors propose a novel approach that combines Conditional Learning and Proximal Policy Optimization (PPO) to address this challenge. The study conducted experiments in an urban scenario with T-type intersections using the CARLA Simulator, showing that the proposed approach is feasible and performs well in achieving the goal of turning at intersections. The research also highlights the potential benefits of autonomous vehicles in improving traffic safety, mobility for elderly and disabled individuals, energy efficiency, and productivity. The paper discusses the lack of interpretability in AI systems and how it affects performance, proposing solutions such as Conditional Imitation Learning and augmentation data. The use of Proximal Policy Optimization (PPO) and augmentation data is emphasized to improve sample efficiency and generalization in end-to-end self-driving tasks. The research also delves into the training process, evaluation, and performance analysis of different reinforcement learning agents, showcasing the effectiveness of PPO agents with augmentation in improving driving performance in a simulated environment. Overall, the paper presents a comprehensive approach to enhancing autonomous driving capabilities through artificial intelligence techniques, contributing to the advancement of self-driving technology."
      ],
      "metadata": {
        "id": "w3ZQmliYTJMU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xk0QsUKF78R6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}